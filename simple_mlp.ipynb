{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_mlp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrQ7MiP7BaiWVKwvQFVrAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TakshPanchal/jax_notebooks/blob/main/simple_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement two hidden layers neural network classifier from scratch in JAX [20 Marks]\n",
        "\n",
        "- [ ] Two hidden layers here means (input - hidden1 - hidden2 - output).\n",
        "- [X]  You must not use flax, optax, or any other library for this task.\n",
        "- [X]  Use MNIST dataset with 80:20 train:test split.\n",
        "- [ ]  Manually optimize the number of neurons in hidden layers.\n",
        "- [ ]  Use gradient descent from scratch to optimize your network. You should use the Pytree concept of JAX to do this elegantly.\n",
        "- [ ]  Plot loss v/s iterations curve with matplotlib.\n",
        "- [ ]  Evaluate the model on test data with various classification metrics and briefly discuss their implications.\n"
      ],
      "metadata": {
        "id": "0wp4EsyoYG5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement NN\n",
        "# TODO: implement GD\n",
        "# TODO: add dataloading\n",
        "# TODO: add training loop, loss fn"
      ],
      "metadata": {
        "id": "S9TkugY2r_we"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KfPyDss_YB3p"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as np\n",
        "from jax import random\n",
        "import jax\n",
        "from jax import grad, jit, vmap\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(layer_sizes,seed):\n",
        "    n_layers = len(layer_sizes)\n",
        "    parent_key = random.PRNGKey(seed)\n",
        "    params = []\n",
        "    keys = random.split(parent_key,num = n_layers - 1)\n",
        "\n",
        "    for i in range(n_layers-1):\n",
        "        w_key,b_key = random.split(keys[i])\n",
        "        in_size = layer_sizes[i]\n",
        "        out_size = layer_sizes[i+1]\n",
        "\n",
        "        W = random.normal(w_key,shape=(in_size,out_size))\n",
        "        b = random.normal(b_key,shape=(out_size,))\n",
        "        params.append([W,b])\n",
        "    return params"
      ],
      "metadata": {
        "id": "YBkR6gpGCT1W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(activation,W,b):\n",
        "    z = np.dot(activation,W) + b \n",
        "    cache = (activation,W,b)\n",
        "    return z, cache"
      ],
      "metadata": {
        "id": "PJRVZiPNCZE5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(activation, W, b, activation_fn):\n",
        "    z, linear_cache = linear_forward(activation, W, b)\n",
        "    activation = activation_fn(z)\n",
        "\n",
        "    return activation, linear_cache"
      ],
      "metadata": {
        "id": "Qs979hrKChll"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def forward(params,x):\n",
        "    \"\"\"\n",
        "        forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "        Arguments:\n",
        "        X -- data, numpy array of shape (input size, number of examples)\n",
        "        parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "        Returns:\n",
        "        AL -- last post-activation value\n",
        "        caches -- list of caches containing:\n",
        "        every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "        the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "    activation = x\n",
        "    caches = []\n",
        "\n",
        "    hidden_layers = params[:-1]\n",
        "\n",
        "    # Loop over the ReLU hidden layers\n",
        "    for W, b in hidden_layers:\n",
        "        activation, cache = linear_activation_forward(activation, W, b, activation_fn = jax.nn.relu)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Perform final trafo to logits\n",
        "    final_W, final_b = params[-1]\n",
        "    logits, cache = linear_activation_forward(activation, final_W, final_b, activation_fn = jax.nn.softmax)\n",
        "    output = jax.nn.softmax(logits)\n",
        "    caches.append(cache)\n",
        "\n",
        "    return output,caches"
      ],
      "metadata": {
        "id": "dC_96qWyCk_O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_forward = jit(vmap(forward, in_axes=(None, 0)))"
      ],
      "metadata": {
        "id": "YSV6jwkHCrp_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def loss(prediction, targets):\n",
        "    \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
        "    # # preds = batch_forward(params, in_arrays)\n",
        "    # -np.mean(np.sum(np.sum(, axis=1), axis=1))\n",
        "\n",
        "    # return -np.sum(prediction * targets)\n",
        "    return -np.sum(targets * np.log(prediction))"
      ],
      "metadata": {
        "id": "vIUPwpyDCxVU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_backward(activation, w, b):\n",
        "    # activation, w, b = cache\n",
        "    (d_activation, dw, db),_ = grad(linear_forward, argnums = (0,1,2),has_aux=True)(activation, w, b)\n",
        "    \n",
        "    return (d_activation, dw, db)\n",
        "\n",
        "batched_layer_backward = vmap(linear_forward,in_axes=(0,None,None),out_axes=)"
      ],
      "metadata": {
        "id": "_HYbw3rDC4fN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "03719048-9c88-4d7e-cc83-ae18ccdff940"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-80828cea9375>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    batched_layer_backward = vmap(linear_forward,in_axes=(0,None,None),out_axes=)\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def linear_activation_backward(cache, activation_fn):\n",
        "#     \"\"\"\n",
        "#     Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "#     Arguments:\n",
        "#     dA -- post-activation gradient for current layer l \n",
        "#     cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "#     activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "#     Returns:\n",
        "#     dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "#     dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "#     db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # I dont need it - skip dz part\n",
        "#     dz = grad(activation_fn)(z)    \n",
        "    \n",
        "#     d_activation, dw, db = linear_backward(linear_cache)\n",
        "    \n",
        "#     return d_activation, dw, db"
      ],
      "metadata": {
        "id": "Zj28-lYaH5RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = []\n",
        "    n_layers = len(caches) # the number of layers\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        activation, w, b = caches[i]\n",
        "        d_activation, d_params  = batched_layer_backward(activation, w, b)\n",
        "        # dW, db = d_params\n",
        "        print(len(d_params))\n",
        "        grads.append((d_activation, dW, db))\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "7k5bcj78KxkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = load_digits()\n",
        "images = mnist['images']\n",
        "labels = mnist['target']"
      ],
      "metadata": {
        "id": "PVY0YjSzNlOl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 10, figsize=(10, 6))\n",
        "for i in range(20):\n",
        "    axes[i//10, i %10].imshow(mnist.images[i], cmap='gray');\n",
        "    axes[i//10, i %10].axis('off')\n",
        "    axes[i//10, i %10].set_title(f\"target: {mnist.target[i]}\")\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "QZ43UUuMwZy_",
        "outputId": "eaba7a2a-7c63-4320-d63b-14dd5992d629"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_images = images.shape[0]\n",
        "n = 10\n",
        "images = jax.device_put(images)  # push NumPy explicitly to GPU\n",
        "labels = jax.device_put(labels) \n",
        "images = images.reshape(total_images,-1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kWWvM68Hvxqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd249d6d-780d-4b35-e1de-cbbf024e3904"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "Yizb8-q7P2tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: make model class\n",
        "epochs = 1\n",
        "layers = [X_train.shape[-1],512,256,n]\n",
        "\n",
        "params = init_params(layers,42)\n",
        "batch_size = 32\n",
        "no_of_samples = X_train.shape[0]\n",
        "no_of_batches = no_of_samples // batch_size\n",
        "\n",
        "for e in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch_i in range(0,no_of_samples,batch_size):\n",
        "        batch_end_idx = batch_i + 32 if batch_i + 32 < no_of_samples else -1\n",
        "        batch_images = images[batch_i: batch_end_idx]\n",
        "        batch_labels = labels[batch_i: batch_end_idx]\n",
        "        batch_labels_encoded = jax.nn.one_hot(batch_labels, len(mnist['target_names']))\n",
        "        # forward pass\n",
        "        batch_predictions,caches = batch_forward(params,batch_images)\n",
        "        # loss calculation\n",
        "        batch_loss = loss(batch_predictions,batch_labels_encoded)\n",
        "        total_loss += batch_loss\n",
        "        # back prop and gradient calculation\n",
        "        backward(caches)\n",
        "\n",
        "        # update()\n",
        "    print(total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "vmnCojJVOOvK",
        "outputId": "de45ae07-0a50-4d94-c745-a31522a02bab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a0dcb6f046e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# back prop and gradient calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# update()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'backward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "9WMDl13ObGMC",
        "outputId": "d9dd9e2d-06fe-477f-a07c-1b3c3287bb0d"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-266-5d5aadd4c3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'caches' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fVoGbFmPbGF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QX-8UXtsbGC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g89G1CgubFyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def MLPClassfier():\n",
        "#     def __init__(self,layer_sizes,seed):\n",
        "#         self.parent_key = random.PRNGKey(seed)\n",
        "#         self.params = []\n",
        "#         self.layer_sizes = layer_sizes\n",
        "#         self.n_layers = len(self.layer_sizes)\n",
        "#         self._init_params()\n",
        "#         self.caches = []\n",
        "\n",
        "#     def _init_params(self,scale = 0.01):\n",
        "#         keys = random.split(self.parent_key,num = self.n_layers - 1)\n",
        "\n",
        "#         for i in range(self.n_layers-1):\n",
        "#             w_key,b_key = random.split(keys[i])\n",
        "#             in_size = self.layer_sizes[i]\n",
        "#             out_size = self.layer_sizes[i+1]\n",
        "\n",
        "#             w = random.normal(w_key,shape=(in_size,out_size))\n",
        "#             b = random.normal(b_key,shape=(out_size,))\n",
        "#             self.params.append([w,b])\n",
        "\n",
        "#     def _linear_forward(self,activation,w,b):\n",
        "#         \"\"\"\n",
        "#             for a single sample\n",
        "#         \"\"\"\n",
        "#         z = np.dot(w,activation) + b \n",
        "#         cache = (activation,w,b)\n",
        "#         return z, cache\n",
        "    \n",
        "#     def _linear_activation_forward(self,activation, w, b, activation_fn):\n",
        "#         z, linear_cache = self._linear_forward(activation, w, b)\n",
        "#         activation = activation_fn(z)\n",
        "#         cache = (linear_cache, z)\n",
        "\n",
        "#         return activation, cache\n",
        "\n",
        "#     @jit\n",
        "#     def _forward(self,x):\n",
        "#         \"\"\"\n",
        "#         forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "        \n",
        "#         Arguments:\n",
        "#         X -- data, numpy array of shape (input size, number of examples)\n",
        "#         parameters -- output of initialize_parameters_deep()\n",
        "        \n",
        "#         Returns:\n",
        "#         AL -- last post-activation value\n",
        "#         caches -- list of caches containing:\n",
        "#                     every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "#                     the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "#         \"\"\"\n",
        "\n",
        "       \n",
        "#         activation = x\n",
        "#         # self.\n",
        "#         L = self.n_layers                # number of layers in the neural network\n",
        "    \n",
        "\n",
        "#         hidden_layers = params[:-1]\n",
        "\n",
        "#         # Loop over the ReLU hidden layers\n",
        "#         for w, b in hidden_layers:\n",
        "#             activation, cache = self._linear_activation_forward(activation, w, b, activation_fn = jax.nn.relu)\n",
        "#             self.caches.append(cache)\n",
        "\n",
        "#         # Perform final trafo to logits\n",
        "#         final_w, final_b = params[-1]\n",
        "#         logits = np.dot(final_w, activation) + final_b\n",
        "#         output, cache = self._linear_activation_forward(activation, final_w, final_b, activation_fn = jax.nn.softmax)\n",
        "#         output = jax.nn.softmax(logits)\n",
        "#         self.caches.append(cache)\n",
        "\n",
        "#         return output\n",
        "\n",
        "#     def forward(self,X):\n",
        "#         batch_forward = jit(vmap(self._forward, in_axes=(None, 0), out_axes=0))\n",
        "#         return batch_forward(X)\n",
        "\n",
        "#     def linear_backward(self, dZ, cache):\n",
        "#         \"\"\"\n",
        "#         Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "#         Arguments:\n",
        "#         dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "#         cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "#         Returns:\n",
        "#         dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "#         dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "#         db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "#         \"\"\"\n",
        "#         A_prev, W, b = cache\n",
        "#         m = A_prev.shape[1]\n",
        "\n",
        "#         ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
        "#         dW = np.dot(dZ, cache[0].T) / m\n",
        "#         db = np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
        "#         dA_prev = np.dot(cache[1].T, dZ)\n",
        "#         ### END CODE HERE ###\n",
        "        \n",
        "#         assert (dA_prev.shape == A_prev.shape)\n",
        "#         assert (dW.shape == W.shape)\n",
        "#         assert (isinstance(db, float))\n",
        "        \n",
        "#         return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "-fj3-RY18knO",
        "cellView": "form"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "@jit\n",
        "def forward(params,in_array):\n",
        "    \"\"\"\n",
        "        Compute the forward pass for each example individually\n",
        "        params: list of weights and biases of MLP \n",
        "        x: single flat image\n",
        "    \"\"\"\n",
        "    hidden_layers = params[:-1]\n",
        "\n",
        "    activations = in_array\n",
        "\n",
        "    # Loop over the ReLU hidden layers\n",
        "    for w, b in hidden_layers:\n",
        "        a = np.dot(w,activations) + b \n",
        "        activation = jax.nn.relu(a)\n",
        "    \n",
        "    # Perform final trafo to logits\n",
        "    final_w, final_b = params[-1]\n",
        "    logits = np.dot(final_w, activations) + final_b\n",
        "    output = jax.nn.softmax(logits)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "oilnpAgI5Bya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(params,imgs,labels):\n",
        "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
        "    value, grads = value_and_grad(loss)(params, x, y)\n"
      ],
      "metadata": {
        "id": "uzSY59J_P8th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_MLPClassfier(layer_sizes,parent_key,scale = 0.01):\n",
        "    params = []\n",
        "    keys = random.split(parent_key,num= len(layer_sizes) - 1)\n",
        "\n",
        "    for i in range(len(layer_sizes)-1):\n",
        "        w_key,b_key = random.split(keys[i])\n",
        "        in_size = layer_sizes[i]\n",
        "        out_size = layer_sizes[i+1]\n",
        "        # print()\n",
        "        w = random.normal(w_key,shape=(in_size,out_size))\n",
        "        b = random.normal(b_key,shape=(out_size,))\n",
        "        params.append([w,b])\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "2mBJQ5mR4jWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "input,h1,h2,output = (784,512,256,10)\n",
        "layers = [input,h1,h2,output]\n",
        "seed = 42 \n",
        "\n",
        "parent_key = random.PRNGKey(seed)\n",
        "MLP_params = init_MLPClassfier(layers,parent_key)\n",
        "\n",
        "print(jax.tree_map(lambda x: x.shape,MLP_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw5CXGtK41Zk",
        "outputId": "29ef9037-b4ed-4e68-e05f-d7704b43bb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(784, 512), (512,)], [(512, 256), (256,)], [(256, 10), (10,)]]\n"
          ]
        }
      ]
    }
  ]
}